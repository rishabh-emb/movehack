{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_area_rel = pd.read_csv(\"Final_speed_area_file_1000_custom.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = speed_area_rel.iloc[:, 2:-1].values\n",
    "y = speed_area_rel.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.18, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor(random_state=0, booster=\"gbtree\", learning_rate=0.18)\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444.818"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.sqrt(mean_squared_error(y_test, xg_pred)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishabh/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train1 = mms.fit_transform(X_train)\n",
    "y_train1 = mms.fit_transform(y_train)\n",
    "\n",
    "mmst = MinMaxScaler()\n",
    "X_test1 = mmst.fit_transform(X_test)\n",
    "y_test1 = mmst.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=90, input_dim=1000, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.6))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.5627\n",
      "Epoch 2/160\n",
      "800/800 [==============================] - 0s 243us/step - loss: 0.2108\n",
      "Epoch 3/160\n",
      "800/800 [==============================] - 0s 260us/step - loss: 0.1917\n",
      "Epoch 4/160\n",
      "800/800 [==============================] - 0s 254us/step - loss: 0.1855\n",
      "Epoch 5/160\n",
      "800/800 [==============================] - 0s 241us/step - loss: 0.1799\n",
      "Epoch 6/160\n",
      "800/800 [==============================] - 0s 243us/step - loss: 0.1825\n",
      "Epoch 7/160\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.1735\n",
      "Epoch 8/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1716\n",
      "Epoch 9/160\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.1677\n",
      "Epoch 10/160\n",
      "800/800 [==============================] - 0s 220us/step - loss: 0.1761\n",
      "Epoch 11/160\n",
      "800/800 [==============================] - 0s 249us/step - loss: 0.1701\n",
      "Epoch 12/160\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.1654\n",
      "Epoch 13/160\n",
      "800/800 [==============================] - 0s 230us/step - loss: 0.1655\n",
      "Epoch 14/160\n",
      "800/800 [==============================] - 0s 234us/step - loss: 0.1604\n",
      "Epoch 15/160\n",
      "800/800 [==============================] - 0s 205us/step - loss: 0.1628\n",
      "Epoch 16/160\n",
      "800/800 [==============================] - 0s 221us/step - loss: 0.1576\n",
      "Epoch 17/160\n",
      "800/800 [==============================] - 0s 240us/step - loss: 0.1656\n",
      "Epoch 18/160\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.1610\n",
      "Epoch 19/160\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.1587\n",
      "Epoch 20/160\n",
      "800/800 [==============================] - 0s 216us/step - loss: 0.1593\n",
      "Epoch 21/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1531\n",
      "Epoch 22/160\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.1516\n",
      "Epoch 23/160\n",
      "800/800 [==============================] - 0s 240us/step - loss: 0.1517\n",
      "Epoch 24/160\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.1525\n",
      "Epoch 25/160\n",
      "800/800 [==============================] - 0s 237us/step - loss: 0.1501\n",
      "Epoch 26/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1539\n",
      "Epoch 27/160\n",
      "800/800 [==============================] - 0s 245us/step - loss: 0.1463\n",
      "Epoch 28/160\n",
      "800/800 [==============================] - 0s 250us/step - loss: 0.1504\n",
      "Epoch 29/160\n",
      "800/800 [==============================] - 0s 240us/step - loss: 0.1509\n",
      "Epoch 30/160\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.1519\n",
      "Epoch 31/160\n",
      "800/800 [==============================] - 0s 246us/step - loss: 0.1481\n",
      "Epoch 32/160\n",
      "800/800 [==============================] - 0s 328us/step - loss: 0.1448\n",
      "Epoch 33/160\n",
      "800/800 [==============================] - 0s 252us/step - loss: 0.1468\n",
      "Epoch 34/160\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.1456\n",
      "Epoch 35/160\n",
      "800/800 [==============================] - 0s 237us/step - loss: 0.1381\n",
      "Epoch 36/160\n",
      "800/800 [==============================] - 0s 246us/step - loss: 0.1429\n",
      "Epoch 37/160\n",
      "800/800 [==============================] - 0s 235us/step - loss: 0.1402\n",
      "Epoch 38/160\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.1393\n",
      "Epoch 39/160\n",
      "800/800 [==============================] - 0s 235us/step - loss: 0.1419\n",
      "Epoch 40/160\n",
      "800/800 [==============================] - 0s 233us/step - loss: 0.1407\n",
      "Epoch 41/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1446\n",
      "Epoch 42/160\n",
      "800/800 [==============================] - 0s 241us/step - loss: 0.1409\n",
      "Epoch 43/160\n",
      "800/800 [==============================] - 0s 241us/step - loss: 0.1428\n",
      "Epoch 44/160\n",
      "800/800 [==============================] - 0s 203us/step - loss: 0.1405\n",
      "Epoch 45/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1380\n",
      "Epoch 46/160\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.1395\n",
      "Epoch 47/160\n",
      "800/800 [==============================] - 0s 241us/step - loss: 0.1386\n",
      "Epoch 48/160\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.1384\n",
      "Epoch 49/160\n",
      "800/800 [==============================] - 0s 221us/step - loss: 0.1392\n",
      "Epoch 50/160\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.1386\n",
      "Epoch 51/160\n",
      "800/800 [==============================] - 0s 251us/step - loss: 0.1391\n",
      "Epoch 52/160\n",
      "800/800 [==============================] - 0s 223us/step - loss: 0.1359 0s - loss: 0.139\n",
      "Epoch 53/160\n",
      "800/800 [==============================] - 0s 245us/step - loss: 0.1389\n",
      "Epoch 54/160\n",
      "800/800 [==============================] - 0s 251us/step - loss: 0.1377\n",
      "Epoch 55/160\n",
      "800/800 [==============================] - 0s 255us/step - loss: 0.1365\n",
      "Epoch 56/160\n",
      "800/800 [==============================] - 0s 237us/step - loss: 0.1394\n",
      "Epoch 57/160\n",
      "800/800 [==============================] - 0s 239us/step - loss: 0.1384\n",
      "Epoch 58/160\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.1331\n",
      "Epoch 59/160\n",
      "800/800 [==============================] - 0s 231us/step - loss: 0.1390\n",
      "Epoch 60/160\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.1373\n",
      "Epoch 61/160\n",
      "800/800 [==============================] - 0s 254us/step - loss: 0.1376\n",
      "Epoch 62/160\n",
      "800/800 [==============================] - 0s 271us/step - loss: 0.1388\n",
      "Epoch 63/160\n",
      "800/800 [==============================] - 0s 229us/step - loss: 0.1390\n",
      "Epoch 64/160\n",
      "800/800 [==============================] - 0s 262us/step - loss: 0.1363\n",
      "Epoch 65/160\n",
      "800/800 [==============================] - 0s 253us/step - loss: 0.1336\n",
      "Epoch 66/160\n",
      "800/800 [==============================] - 0s 255us/step - loss: 0.1338\n",
      "Epoch 67/160\n",
      "800/800 [==============================] - 0s 253us/step - loss: 0.1328\n",
      "Epoch 68/160\n",
      "800/800 [==============================] - 0s 250us/step - loss: 0.1347\n",
      "Epoch 69/160\n",
      "800/800 [==============================] - 0s 282us/step - loss: 0.1309\n",
      "Epoch 70/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1346\n",
      "Epoch 71/160\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.1313\n",
      "Epoch 72/160\n",
      "800/800 [==============================] - 0s 242us/step - loss: 0.1302\n",
      "Epoch 73/160\n",
      "800/800 [==============================] - 0s 218us/step - loss: 0.1335\n",
      "Epoch 74/160\n",
      "800/800 [==============================] - 0s 239us/step - loss: 0.1299\n",
      "Epoch 75/160\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.1304\n",
      "Epoch 76/160\n",
      "800/800 [==============================] - 0s 265us/step - loss: 0.1276\n",
      "Epoch 77/160\n",
      "800/800 [==============================] - 0s 358us/step - loss: 0.1265\n",
      "Epoch 78/160\n",
      "800/800 [==============================] - 0s 268us/step - loss: 0.1297\n",
      "Epoch 79/160\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.1338\n",
      "Epoch 80/160\n",
      "800/800 [==============================] - 0s 211us/step - loss: 0.1313\n",
      "Epoch 81/160\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.1293\n",
      "Epoch 82/160\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.1292\n",
      "Epoch 83/160\n",
      "800/800 [==============================] - 0s 235us/step - loss: 0.1285\n",
      "Epoch 84/160\n",
      "800/800 [==============================] - 0s 217us/step - loss: 0.1306\n",
      "Epoch 85/160\n",
      "800/800 [==============================] - 0s 222us/step - loss: 0.1272\n",
      "Epoch 86/160\n",
      "800/800 [==============================] - 0s 245us/step - loss: 0.1274\n",
      "Epoch 87/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1332\n",
      "Epoch 88/160\n",
      "800/800 [==============================] - 0s 241us/step - loss: 0.1301\n",
      "Epoch 89/160\n",
      "800/800 [==============================] - 0s 240us/step - loss: 0.1243\n",
      "Epoch 90/160\n",
      "800/800 [==============================] - 0s 234us/step - loss: 0.1290\n",
      "Epoch 91/160\n",
      "800/800 [==============================] - 0s 211us/step - loss: 0.1295\n",
      "Epoch 92/160\n",
      "800/800 [==============================] - 0s 221us/step - loss: 0.1291\n",
      "Epoch 93/160\n",
      "800/800 [==============================] - 0s 223us/step - loss: 0.1286\n",
      "Epoch 94/160\n",
      "800/800 [==============================] - 0s 214us/step - loss: 0.1258\n",
      "Epoch 95/160\n",
      "800/800 [==============================] - 0s 231us/step - loss: 0.1255\n",
      "Epoch 96/160\n",
      "800/800 [==============================] - 0s 205us/step - loss: 0.1271\n",
      "Epoch 97/160\n",
      "800/800 [==============================] - 0s 220us/step - loss: 0.1265\n",
      "Epoch 98/160\n",
      "800/800 [==============================] - 0s 207us/step - loss: 0.1290\n",
      "Epoch 99/160\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.1254\n",
      "Epoch 100/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1287\n",
      "Epoch 101/160\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.1248\n",
      "Epoch 102/160\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.1247\n",
      "Epoch 103/160\n",
      "800/800 [==============================] - 0s 204us/step - loss: 0.1223\n",
      "Epoch 104/160\n",
      "800/800 [==============================] - 0s 211us/step - loss: 0.1222\n",
      "Epoch 105/160\n",
      "800/800 [==============================] - 0s 233us/step - loss: 0.1240\n",
      "Epoch 106/160\n",
      "800/800 [==============================] - 0s 238us/step - loss: 0.1188\n",
      "Epoch 107/160\n",
      "800/800 [==============================] - 0s 232us/step - loss: 0.1171\n",
      "Epoch 108/160\n",
      "800/800 [==============================] - 0s 222us/step - loss: 0.1197\n",
      "Epoch 109/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1172\n",
      "Epoch 110/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1218\n",
      "Epoch 111/160\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.1215\n",
      "Epoch 112/160\n",
      "800/800 [==============================] - 0s 214us/step - loss: 0.1214\n",
      "Epoch 113/160\n",
      "800/800 [==============================] - 0s 208us/step - loss: 0.1166\n",
      "Epoch 114/160\n",
      "800/800 [==============================] - 0s 212us/step - loss: 0.1203\n",
      "Epoch 115/160\n",
      "800/800 [==============================] - 0s 218us/step - loss: 0.1241\n",
      "Epoch 116/160\n",
      "800/800 [==============================] - 0s 247us/step - loss: 0.1215\n",
      "Epoch 117/160\n",
      "800/800 [==============================] - 0s 248us/step - loss: 0.1178\n",
      "Epoch 118/160\n",
      "800/800 [==============================] - 0s 247us/step - loss: 0.1240\n",
      "Epoch 119/160\n",
      "800/800 [==============================] - 0s 233us/step - loss: 0.1204\n",
      "Epoch 120/160\n",
      "800/800 [==============================] - 0s 225us/step - loss: 0.1193\n",
      "Epoch 121/160\n",
      "800/800 [==============================] - 0s 217us/step - loss: 0.1226\n",
      "Epoch 122/160\n",
      "800/800 [==============================] - 0s 235us/step - loss: 0.1189\n",
      "Epoch 123/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1191\n",
      "Epoch 124/160\n",
      "800/800 [==============================] - 0s 233us/step - loss: 0.1175\n",
      "Epoch 125/160\n",
      "800/800 [==============================] - 0s 212us/step - loss: 0.1183\n",
      "Epoch 126/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1153\n",
      "Epoch 127/160\n",
      "800/800 [==============================] - 0s 214us/step - loss: 0.1221\n",
      "Epoch 128/160\n",
      "800/800 [==============================] - 0s 205us/step - loss: 0.1174\n",
      "Epoch 129/160\n",
      "800/800 [==============================] - 0s 228us/step - loss: 0.1182\n",
      "Epoch 130/160\n",
      "800/800 [==============================] - 0s 237us/step - loss: 0.1155\n",
      "Epoch 131/160\n",
      "800/800 [==============================] - 0s 288us/step - loss: 0.1227\n",
      "Epoch 132/160\n",
      "800/800 [==============================] - 0s 279us/step - loss: 0.1168\n",
      "Epoch 133/160\n",
      "800/800 [==============================] - 0s 272us/step - loss: 0.1173\n",
      "Epoch 134/160\n",
      "800/800 [==============================] - 0s 234us/step - loss: 0.1163\n",
      "Epoch 135/160\n",
      "800/800 [==============================] - 0s 187us/step - loss: 0.1171\n",
      "Epoch 136/160\n",
      "800/800 [==============================] - 0s 210us/step - loss: 0.1113\n",
      "Epoch 137/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1140\n",
      "Epoch 138/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1124\n",
      "Epoch 139/160\n",
      "800/800 [==============================] - 0s 218us/step - loss: 0.1133\n",
      "Epoch 140/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1149\n",
      "Epoch 141/160\n",
      "800/800 [==============================] - 0s 220us/step - loss: 0.1148\n",
      "Epoch 142/160\n",
      "800/800 [==============================] - 0s 205us/step - loss: 0.1133\n",
      "Epoch 143/160\n",
      "800/800 [==============================] - 0s 211us/step - loss: 0.1160\n",
      "Epoch 144/160\n",
      "800/800 [==============================] - 0s 239us/step - loss: 0.1137\n",
      "Epoch 145/160\n",
      "800/800 [==============================] - 0s 246us/step - loss: 0.1140\n",
      "Epoch 146/160\n",
      "800/800 [==============================] - 0s 212us/step - loss: 0.1129\n",
      "Epoch 147/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1128\n",
      "Epoch 148/160\n",
      "800/800 [==============================] - 0s 219us/step - loss: 0.1127\n",
      "Epoch 149/160\n",
      "800/800 [==============================] - 0s 216us/step - loss: 0.1127\n",
      "Epoch 150/160\n",
      "800/800 [==============================] - 0s 236us/step - loss: 0.1145\n",
      "Epoch 151/160\n",
      "800/800 [==============================] - 0s 224us/step - loss: 0.1128\n",
      "Epoch 152/160\n",
      "800/800 [==============================] - 0s 212us/step - loss: 0.1141\n",
      "Epoch 153/160\n",
      "800/800 [==============================] - 0s 233us/step - loss: 0.1157\n",
      "Epoch 154/160\n",
      "800/800 [==============================] - 0s 207us/step - loss: 0.1095\n",
      "Epoch 155/160\n",
      "800/800 [==============================] - 0s 218us/step - loss: 0.1114\n",
      "Epoch 156/160\n",
      "800/800 [==============================] - 0s 207us/step - loss: 0.1090\n",
      "Epoch 157/160\n",
      "800/800 [==============================] - 0s 227us/step - loss: 0.1137\n",
      "Epoch 158/160\n",
      "800/800 [==============================] - 0s 208us/step - loss: 0.1101\n",
      "Epoch 159/160\n",
      "800/800 [==============================] - 0s 223us/step - loss: 0.1093\n",
      "Epoch 160/160\n",
      "800/800 [==============================] - 0s 217us/step - loss: 0.1114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa33e9d8668>"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train1, y_train1, batch_size=16, epochs=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred = model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred = mmst.inverse_transform(ann_pred)\n",
    "y_test1 = mmst.inverse_transform(y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565.042"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.sqrt(mean_squared_error(y_test1, ann_pred)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred1 = model.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred1 = mms.inverse_transform(ann_pred1)\n",
    "y_train1 = mms.inverse_transform(y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391.844"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.sqrt(mean_squared_error(y_train1, ann_pred1)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'y_test': np.reshape(y_test1, (200)), 'ann_pred': np.reshape(ann_pred, (200)),\\\n",
    "                    'xg_boost': xg_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ann_pred</th>\n",
       "      <th>xg_boost</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3527.736084</td>\n",
       "      <td>3679.524170</td>\n",
       "      <td>3464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2924.302734</td>\n",
       "      <td>3411.730957</td>\n",
       "      <td>3216.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3525.947021</td>\n",
       "      <td>3788.846680</td>\n",
       "      <td>4353.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3605.743652</td>\n",
       "      <td>3622.436035</td>\n",
       "      <td>4048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3382.592041</td>\n",
       "      <td>3079.441162</td>\n",
       "      <td>3609.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3730.540283</td>\n",
       "      <td>3793.164062</td>\n",
       "      <td>3176.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3543.276611</td>\n",
       "      <td>3457.833008</td>\n",
       "      <td>3652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3364.839844</td>\n",
       "      <td>3401.435791</td>\n",
       "      <td>3243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2986.868164</td>\n",
       "      <td>3507.072998</td>\n",
       "      <td>3308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3046.864990</td>\n",
       "      <td>3077.898926</td>\n",
       "      <td>3485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2933.334473</td>\n",
       "      <td>2976.635254</td>\n",
       "      <td>3255.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4059.943115</td>\n",
       "      <td>3622.452393</td>\n",
       "      <td>3690.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3433.622070</td>\n",
       "      <td>3619.371338</td>\n",
       "      <td>3234.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3442.679199</td>\n",
       "      <td>4343.010742</td>\n",
       "      <td>4917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3453.786865</td>\n",
       "      <td>3257.516846</td>\n",
       "      <td>3945.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3027.925293</td>\n",
       "      <td>2866.436279</td>\n",
       "      <td>2289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3659.396729</td>\n",
       "      <td>3451.049561</td>\n",
       "      <td>3272.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2742.638916</td>\n",
       "      <td>2639.615234</td>\n",
       "      <td>2405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3933.913086</td>\n",
       "      <td>4251.298340</td>\n",
       "      <td>4977.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3805.822754</td>\n",
       "      <td>3878.634033</td>\n",
       "      <td>3796.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2689.514160</td>\n",
       "      <td>3071.844971</td>\n",
       "      <td>3392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3818.665527</td>\n",
       "      <td>3700.711670</td>\n",
       "      <td>3168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2438.181641</td>\n",
       "      <td>2520.918945</td>\n",
       "      <td>2303.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3055.616211</td>\n",
       "      <td>2980.137695</td>\n",
       "      <td>3459.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3732.513672</td>\n",
       "      <td>3645.887207</td>\n",
       "      <td>3602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3543.579590</td>\n",
       "      <td>3664.934570</td>\n",
       "      <td>3215.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3137.400635</td>\n",
       "      <td>3006.751953</td>\n",
       "      <td>2215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3635.144043</td>\n",
       "      <td>4001.254883</td>\n",
       "      <td>4506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3056.864258</td>\n",
       "      <td>3585.654297</td>\n",
       "      <td>3584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3081.838867</td>\n",
       "      <td>3367.595215</td>\n",
       "      <td>3227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3161.719238</td>\n",
       "      <td>3261.897461</td>\n",
       "      <td>3204.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3275.960693</td>\n",
       "      <td>3133.527832</td>\n",
       "      <td>3408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2807.321777</td>\n",
       "      <td>2967.554199</td>\n",
       "      <td>2322.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3425.791992</td>\n",
       "      <td>4038.540771</td>\n",
       "      <td>3279.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3411.357910</td>\n",
       "      <td>3535.669922</td>\n",
       "      <td>3513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3794.055420</td>\n",
       "      <td>3418.491211</td>\n",
       "      <td>3408.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3159.300781</td>\n",
       "      <td>2626.387939</td>\n",
       "      <td>2235.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>4047.488770</td>\n",
       "      <td>3699.163574</td>\n",
       "      <td>3895.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3087.572754</td>\n",
       "      <td>3291.457764</td>\n",
       "      <td>3424.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3724.003174</td>\n",
       "      <td>4212.583496</td>\n",
       "      <td>4636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3864.991455</td>\n",
       "      <td>3854.767090</td>\n",
       "      <td>3202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>4246.813965</td>\n",
       "      <td>3576.209473</td>\n",
       "      <td>3210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3683.162598</td>\n",
       "      <td>3953.393799</td>\n",
       "      <td>3899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>3049.781982</td>\n",
       "      <td>2826.310303</td>\n",
       "      <td>2175.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>3389.819580</td>\n",
       "      <td>3556.169678</td>\n",
       "      <td>3422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>3724.195557</td>\n",
       "      <td>3698.674805</td>\n",
       "      <td>3027.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>3642.203857</td>\n",
       "      <td>3628.437988</td>\n",
       "      <td>3588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>3807.062256</td>\n",
       "      <td>3476.413330</td>\n",
       "      <td>3644.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>3227.654297</td>\n",
       "      <td>2793.633545</td>\n",
       "      <td>3321.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>4263.791016</td>\n",
       "      <td>4580.135742</td>\n",
       "      <td>4943.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>3792.662842</td>\n",
       "      <td>3918.282959</td>\n",
       "      <td>3346.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>3483.986084</td>\n",
       "      <td>3815.626465</td>\n",
       "      <td>3337.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2697.635254</td>\n",
       "      <td>3008.189209</td>\n",
       "      <td>2607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2803.951416</td>\n",
       "      <td>3240.366943</td>\n",
       "      <td>2269.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>3390.425537</td>\n",
       "      <td>3800.233887</td>\n",
       "      <td>3268.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>3196.188965</td>\n",
       "      <td>3469.970215</td>\n",
       "      <td>4198.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3548.394043</td>\n",
       "      <td>2840.281738</td>\n",
       "      <td>3689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>2915.045410</td>\n",
       "      <td>2875.180908</td>\n",
       "      <td>2303.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>3873.874512</td>\n",
       "      <td>3811.276611</td>\n",
       "      <td>3884.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>3337.908936</td>\n",
       "      <td>2892.463135</td>\n",
       "      <td>2292.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ann_pred     xg_boost  y_test\n",
       "0    3527.736084  3679.524170  3464.0\n",
       "1    2924.302734  3411.730957  3216.5\n",
       "2    3525.947021  3788.846680  4353.5\n",
       "3    3605.743652  3622.436035  4048.0\n",
       "4    3382.592041  3079.441162  3609.5\n",
       "5    3730.540283  3793.164062  3176.5\n",
       "6    3543.276611  3457.833008  3652.0\n",
       "7    3364.839844  3401.435791  3243.0\n",
       "8    2986.868164  3507.072998  3308.0\n",
       "9    3046.864990  3077.898926  3485.0\n",
       "10   2933.334473  2976.635254  3255.5\n",
       "11   4059.943115  3622.452393  3690.5\n",
       "12   3433.622070  3619.371338  3234.5\n",
       "13   3442.679199  4343.010742  4917.0\n",
       "14   3453.786865  3257.516846  3945.5\n",
       "15   3027.925293  2866.436279  2289.0\n",
       "16   3659.396729  3451.049561  3272.5\n",
       "17   2742.638916  2639.615234  2405.0\n",
       "18   3933.913086  4251.298340  4977.5\n",
       "19   3805.822754  3878.634033  3796.5\n",
       "20   2689.514160  3071.844971  3392.0\n",
       "21   3818.665527  3700.711670  3168.0\n",
       "22   2438.181641  2520.918945  2303.5\n",
       "23   3055.616211  2980.137695  3459.5\n",
       "24   3732.513672  3645.887207  3602.0\n",
       "25   3543.579590  3664.934570  3215.5\n",
       "26   3137.400635  3006.751953  2215.0\n",
       "27   3635.144043  4001.254883  4506.0\n",
       "28   3056.864258  3585.654297  3584.0\n",
       "29   3081.838867  3367.595215  3227.0\n",
       "..           ...          ...     ...\n",
       "170  3161.719238  3261.897461  3204.5\n",
       "171  3275.960693  3133.527832  3408.0\n",
       "172  2807.321777  2967.554199  2322.5\n",
       "173  3425.791992  4038.540771  3279.5\n",
       "174  3411.357910  3535.669922  3513.0\n",
       "175  3794.055420  3418.491211  3408.5\n",
       "176  3159.300781  2626.387939  2235.5\n",
       "177  4047.488770  3699.163574  3895.5\n",
       "178  3087.572754  3291.457764  3424.5\n",
       "179  3724.003174  4212.583496  4636.0\n",
       "180  3864.991455  3854.767090  3202.0\n",
       "181  4246.813965  3576.209473  3210.0\n",
       "182  3683.162598  3953.393799  3899.0\n",
       "183  3049.781982  2826.310303  2175.5\n",
       "184  3389.819580  3556.169678  3422.0\n",
       "185  3724.195557  3698.674805  3027.5\n",
       "186  3642.203857  3628.437988  3588.0\n",
       "187  3807.062256  3476.413330  3644.5\n",
       "188  3227.654297  2793.633545  3321.5\n",
       "189  4263.791016  4580.135742  4943.5\n",
       "190  3792.662842  3918.282959  3346.5\n",
       "191  3483.986084  3815.626465  3337.5\n",
       "192  2697.635254  3008.189209  2607.0\n",
       "193  2803.951416  3240.366943  2269.5\n",
       "194  3390.425537  3800.233887  3268.5\n",
       "195  3196.188965  3469.970215  4198.5\n",
       "196  3548.394043  2840.281738  3689.0\n",
       "197  2915.045410  2875.180908  2303.5\n",
       "198  3873.874512  3811.276611  3884.5\n",
       "199  3337.908936  2892.463135  2292.0\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
